{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e342763",
   "metadata": {},
   "source": [
    "# UDACITY SageMaker Essentials: Batch Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b3eee",
   "metadata": {},
   "source": [
    "In the last exercise, we asked you to reflect on the disadvantages of having to perform preprocessing on a local machine. In addition to those disadvantages, such as user error and hardware limitations, you may have also encountered another frustration in submitting a large amount of data to an endpoint. There may be network limitations on your end, there may be security/privacy concerns, and there might be an obvious performance advantage in parallelism that may be difficult to implement. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc794fe",
   "metadata": {},
   "source": [
    "Batch transform essentially addresses all of these issues. The primary use case for this is to make an inference on a dataset rather than making many individual calls to an endpoint. AWS SageMaker, similar to other tools that we encountered, does the heavy implementation lifting of reading data and splitting the burden among instances. All that's required of us is to give batch transform the correct directions to the data we want to submit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae54cdc1",
   "metadata": {},
   "source": [
    "Alas, this dataset is unfortunately not quite in the correct format to be properly digested by batch transform. Although this tool is capable of digesting lists of json objects, it is not capable of the processing operations that we would ideally perform on it. So, yet again, we must preprocess data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887aa98f",
   "metadata": {},
   "source": [
    "## Exercise: Preprocess (again, again) and upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692a223d",
   "metadata": {},
   "source": [
    "The cell below provides you two functions. The `split_sentences` preprocesses the reviews and you should be very familiar with function. Remember that the BlazingText expects a input with JSON format, the `cycle_data` formats the review to the following: {'source': 'THIS IS A SAMPLE SENTENCE'} and writes it into a file.\n",
    "\n",
    "Using the cell to complete the following tasks:\n",
    "* preprecessing reviews_Musical_Instruments_5.json \n",
    "* upload the file consisting of the data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecc6a141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import zipfile\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "from src.paths import CODE_DIR, DATA_DIR, RAW_DATA_DIR, TRANSFORMED_DATA_DIR, TEST_DIR\n",
    "\n",
    "# load BUCKET and ROLE from .env file\n",
    "bucket = os.getenv(\"BUCKET\")\n",
    "role = os.getenv(\"ROLE\")\n",
    "region = os.getenv(\"REGION\")\n",
    "\n",
    "S3_LOCATION = f\"s3://{bucket}/1\"\n",
    "\n",
    "# Adding custom folders to the system path for easy import\n",
    "sys.path.extend([str(CODE_DIR)])\n",
    "\n",
    "# Data file path\n",
    "DATA_FILE_PATH = RAW_DATA_DIR / \"reviews_Musical_Instruments_5.json.zip\"\n",
    "OUTPUT_FILE_PATH = TRANSFORMED_DATA_DIR / \"reviews_Musical_Instruments_5.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03829953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /Users/carlos/Projects/aws-ml-engineering/data/transformed/music_instruments_reviews.txt uploaded to sigmoidal-bucket/1/music_instruments_reviews.txt\n",
      "s3://sigmoidal-bucket/1/music_instruments_reviews.txt\n"
     ]
    }
   ],
   "source": [
    "# Input the the file to write the data \n",
    "file_name = \"music_instruments_reviews.txt\"\n",
    "s3_prefix = \"1\"\n",
    "s3_output_path = s3_prefix + '/' + file_name # The key within the bucket\n",
    "\n",
    "\n",
    "# Function below unzips the archive to the local directory. \n",
    "\n",
    "def unzip_data(input_file_path: str, output_file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Unzip the data file\n",
    "    \"\"\"\n",
    "    # extracl all files to the output_file_path, that already includes the file name\n",
    "    with zipfile.ZipFile(input_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_file_path)\n",
    "\n",
    "\n",
    "def split_sentences(input_data):\n",
    "    split_sentences = []\n",
    "    for l in open(input_data, 'r'):\n",
    "        l_object = json.loads(l)\n",
    "        helpful_votes = float(l_object['helpful'][0])\n",
    "        total_votes = l_object['helpful'][1]\n",
    "        if total_votes != 0 and helpful_votes/total_votes != .5:  # Filter out same data as prior jobs. \n",
    "            reviewText = l_object['reviewText']\n",
    "            sentences = reviewText.split(\".\") \n",
    "            for s in sentences:\n",
    "                if s: # Make sure sentences isn't empty. Common w/ \"...\"\n",
    "                    split_sentences.append(s)\n",
    "    return split_sentences\n",
    "\n",
    "\n",
    "# Format the data as {'source': 'THIS IS A SAMPLE SENTENCE'}\n",
    "# And write the data into a file\n",
    "def cycle_data(fp, data):\n",
    "    for d in data:\n",
    "        fp.write(json.dumps({'source':d}) + '\\n')\n",
    "\n",
    "\n",
    "# Uploads a file to an S3 bucket, with optional customizations.\n",
    "def upload_file_to_s3(local_file_path, bucket_name, s3_output_path):\n",
    "    \"\"\"\n",
    "    Uploads a file to an S3 bucket, with optional customizations.\n",
    "\n",
    "    Parameters:\n",
    "    local_file_path (str): The path to the local file to upload.\n",
    "    bucket_name (str): The name of the S3 bucket to upload to.\n",
    "    s3_output_path (str): The output path (key) in the S3 bucket.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # Upload the file\n",
    "    try:\n",
    "        s3_client.upload_file(local_file_path, bucket_name, s3_output_path)\n",
    "        print(f\"File {local_file_path} uploaded to {bucket_name}/{s3_output_path}\")\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "# Unzips file.\n",
    "unzip_data(DATA_FILE_PATH, TRANSFORMED_DATA_DIR)\n",
    "\n",
    "# Todo: preprocess reviews_Musical_Instruments_5.json \n",
    "sentences = split_sentences(OUTPUT_FILE_PATH)\n",
    "\n",
    "# Write data to a file and upload it to s3.\n",
    "with open(str(TRANSFORMED_DATA_DIR / file_name), 'w') as f:\n",
    "    cycle_data(f, sentences)\n",
    "\n",
    "# # upload_file_to_s3(file_name, s3_prefix)\n",
    "upload_file_to_s3(str(TRANSFORMED_DATA_DIR / file_name), bucket, s3_output_path)\n",
    "\n",
    "# Adjust the batch_transform_input_path\n",
    "batch_transform_input_path = f\"s3://{bucket}/{s3_output_path}\"\n",
    "print(batch_transform_input_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724938c",
   "metadata": {},
   "source": [
    "## Exercise: Use Batch Transform to perform an inference on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b34d68",
   "metadata": {},
   "source": [
    "We utilize batch transform through a transformer object. Similar to how we initialized a predictor object in the last exercise, complete the code below to initialize a transformer object and launch a transform job.   \n",
    "\n",
    "You will need the following:\n",
    "\n",
    "* Similar to last exercise, you will need to get a BlazingText image uri from AWS. The methodology you use to do so should be identical to the last exercise.  \n",
    "* You will need to instantiate a \"model\" object.\n",
    "* You will need to call the \"transformer\" method on the model object to create a transformer. We suggest using 1 instance of ml.m4.xlarge. If this isn't available in your region, feel free to use another instance, such as ml.m5.large\n",
    "* You will need to use this transformer on the data we uploaded to s3. You will be able to do so by inserting an \"S3Prefix\" data_type and a \"application/jsonlines\" content_type, split by \"Line\".\n",
    "\n",
    "Consult the following documentation: https://sagemaker.readthedocs.io/en/stable/api/inference/transformer.html\n",
    "\n",
    "End-to-end, this process should take about 5 minutes on the whole dataset. While developing, consider uploading a subset of the data to s3, and evaluate on that instead. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94c8c613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/carlos/Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/carlos/Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/carlos/Library/Application Support/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: blazingtext-2023-12-13-19-02-04-162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............................Arguments: serve\n",
      "[12/13/2023 19:07:41 INFO 140107733665600] Finding and loading model\n",
      "[12/13/2023 19:07:41 INFO 140107733665600] Trying to load model from /opt/ml/model/model.bin\n",
      "[12/13/2023 19:07:42 INFO 140107733665600] Number of server workers: 4\n",
      "[2023-12-13 19:07:42 +0000] [1] [INFO] Starting gunicorn 19.7.1\n",
      "[2023-12-13 19:07:42 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\n",
      "[2023-12-13 19:07:42 +0000] [1] [INFO] Using worker: sync\n",
      "/opt/amazon/python3.8/lib/python3.8/os.py:1023: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  return io.open(fd, *args, **kwargs)\n",
      "[2023-12-13 19:07:42 +0000] [33] [INFO] Booting worker with pid: 33\n",
      "[2023-12-13 19:07:42 +0000] [34] [INFO] Booting worker with pid: 34\n",
      "Arguments: serve\n",
      "[12/13/2023 19:07:41 INFO 140107733665600] Finding and loading model\n",
      "[12/13/2023 19:07:41 INFO 140107733665600] Trying to load model from /opt/ml/model/model.bin\n",
      "[12/13/2023 19:07:42 INFO 140107733665600] Number of server workers: 4\n",
      "[2023-12-13 19:07:42 +0000] [1] [INFO] Starting gunicorn 19.7.1\n",
      "[2023-12-13 19:07:42 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\n",
      "[2023-12-13 19:07:42 +0000] [1] [INFO] Using worker: sync\n",
      "/opt/amazon/python3.8/lib/python3.8/os.py:1023: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  return io.open(fd, *args, **kwargs)\n",
      "[2023-12-13 19:07:42 +0000] [33] [INFO] Booting worker with pid: 33\n",
      "[2023-12-13 19:07:42 +0000] [34] [INFO] Booting worker with pid: 34\n",
      "[2023-12-13 19:07:42 +0000] [35] [INFO] Booting worker with pid: 35\n",
      "[2023-12-13 19:07:42 +0000] [36] [INFO] Booting worker with pid: 36\n",
      "[2023-12-13 19:07:42 +0000] [35] [INFO] Booting worker with pid: 35\n",
      "[2023-12-13 19:07:42 +0000] [36] [INFO] Booting worker with pid: 36\n",
      "2023-12-13T19:07:45.593:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\n",
      "\n",
      "Arguments: serve\n",
      "[12/13/2023 19:07:41 INFO 140107733665600] Finding and loading model\n",
      "[12/13/2023 19:07:41 INFO 140107733665600] Trying to load model from /opt/ml/model/model.bin\n",
      "[12/13/2023 19:07:42 INFO 140107733665600] Number of server workers: 4\n",
      "[2023-12-13 19:07:42 +0000] [1] [INFO] Starting gunicorn 19.7.1\n",
      "[2023-12-13 19:07:42 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\n",
      "[2023-12-13 19:07:42 +0000] [1] [INFO] Using worker: sync\n",
      "/opt/amazon/python3.8/lib/python3.8/os.py:1023: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  return io.open(fd, *args, **kwargs)\n",
      "[2023-12-13 19:07:42 +0000] [33] [INFO] Booting worker with pid: 33\n",
      "[2023-12-13 19:07:42 +0000] [34] [INFO] Booting worker with pid: 34\n",
      "Arguments: serve\n",
      "[12/13/2023 19:07:41 INFO 140107733665600] Finding and loading model\n",
      "[12/13/2023 19:07:41 INFO 140107733665600] Trying to load model from /opt/ml/model/model.bin\n",
      "[12/13/2023 19:07:42 INFO 140107733665600] Number of server workers: 4\n",
      "[2023-12-13 19:07:42 +0000] [1] [INFO] Starting gunicorn 19.7.1\n",
      "[2023-12-13 19:07:42 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\n",
      "[2023-12-13 19:07:42 +0000] [1] [INFO] Using worker: sync\n",
      "/opt/amazon/python3.8/lib/python3.8/os.py:1023: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  return io.open(fd, *args, **kwargs)\n",
      "[2023-12-13 19:07:42 +0000] [33] [INFO] Booting worker with pid: 33\n",
      "[2023-12-13 19:07:42 +0000] [34] [INFO] Booting worker with pid: 34\n",
      "[2023-12-13 19:07:42 +0000] [35] [INFO] Booting worker with pid: 35\n",
      "[2023-12-13 19:07:42 +0000] [36] [INFO] Booting worker with pid: 36\n",
      "[2023-12-13 19:07:42 +0000] [35] [INFO] Booting worker with pid: 35\n",
      "[2023-12-13 19:07:42 +0000] [36] [INFO] Booting worker with pid: 36\n",
      "2023-12-13T19:07:45.593:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import image_uris\n",
    "\n",
    "# Get the image uri using the \"blazingtext\" algorithm in your region. \n",
    "\n",
    "image_uri = image_uris.retrieve(\n",
    "    framework='blazingtext', \n",
    "    region=region\n",
    ")\n",
    "\n",
    "# Get the model artifact from S3\n",
    "model_data = f\"{S3_LOCATION}/model_artifacts/blazingtext-2023-12-13-13-50-26-062/output/model.tar.gz\"\n",
    "\n",
    "# Get the s3 path for the batch transform data\n",
    "batch_transform_output_path = S3_LOCATION + '/batch_transform_output'\n",
    "\n",
    "# Define a model object\n",
    "model = Model(\n",
    "    image_uri=image_uri, \n",
    "    model_data=model_data, \n",
    "    role=role\n",
    ")\n",
    "\n",
    "# Define a transformer object, using a single instance ml.m4.xlarge. Specify an output path to your s3 bucket. \n",
    "\n",
    "transformer = model.transformer(\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m4.xlarge', \n",
    "    output_path=batch_transform_output_path\n",
    ")\n",
    "\n",
    "# Call the transform method. Set content_type='application/jsonlines', split_type='Line'\n",
    "\n",
    "transformer.transform(\n",
    "    data=batch_transform_input_path, \n",
    "    data_type='S3Prefix',\n",
    "    content_type='application/jsonlines', \n",
    "    split_type='Line'\n",
    ")\n",
    "\n",
    "transformer.wait()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
