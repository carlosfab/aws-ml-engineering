{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "345c18af",
   "metadata": {},
   "source": [
    "# UDACITY SageMaker Essentials: Training Job Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea8b4d8",
   "metadata": {},
   "source": [
    "Good job on your work so far! You've gotten an overview of building an ML Workflow in AWS. Now, it's time to practice your skills. In this exercise, you will be training a BlazingText model to help predict the helpfulness of Amazon reviews. The model & parameters have already been chosen for you; it's your task to properly upload the data necessary for the job and launch the training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9e6bfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/carlos/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "import zipfile\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Union, Optional\n",
    "\n",
    "from src.paths import CODE_DIR, DATA_DIR, RAW_DATA_DIR, TRANSFORMED_DATA_DIR, TEST_DIR\n",
    "\n",
    "\n",
    "# Adding custom folders to the system path for easy import\n",
    "sys.path.extend([str(CODE_DIR)])\n",
    "\n",
    "# Data file path\n",
    "DATA_FILE_PATH = RAW_DATA_DIR / \"Toys_and_Games_5.json.zip\"\n",
    "OUTPUT_FILE_PATH = TRANSFORMED_DATA_DIR / \"Toys_and_Games_5.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16340311",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d83e2cb",
   "metadata": {},
   "source": [
    "The data we'll be examining today is a collection of reviews for an assortment of toys and games found on Amazon. This data includes, but is not limited to, the text of the review itself as well as the number of user \"votes\" on whether or not the review was helpful. Today, we will be making a model that predicts the usefulness of a review, given only the text of the review. This is an example of a problem in the domain of supervised sentiment analysis; we are trying to extract something subjective from text given prior labeled text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38f892",
   "metadata": {},
   "source": [
    "Before we get started, we want to know what form of data is accepted in the algorithm we're using. We'll be using BlazingText, an implemention of Word2Vec optimized for SageMaker. In order for this optimization to be effective, data needs to be preprocessed to match the correct format. The documentation for this algorithm can be found here: https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59be0c47",
   "metadata": {},
   "source": [
    "We will be training under \"File Mode\", which requires us to do two things in preprocessing this data. First, we need to generate labels from the votes. For this exercise, if the majority of votes for a review is helpful, we will designate it \\_\\_label\\_\\_1, and if the majority of votes for a review is unhelpful, we will designate it \\_\\_label\\_\\_2. In the edge case where the values are equal, we will drop the review from consideration. Second, we need to separate the sentences, while keeping the original label for the review. These reviews will often consist of several sentences, and this algorithm is optimized to perform best on many small sentences rather than fewer larger paragraphs. We will separate these sentences by the character \".\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b79bab4",
   "metadata": {},
   "source": [
    "(This process is obviously very naive, but we will get remarkable results even without a lot of finetuning!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09129d9c",
   "metadata": {},
   "source": [
    "This preprocessing is done for you in the cells below. Make sure you go through the code and understand what's being done in each step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52fb5427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__1 Love the magnet easel\n",
      "__label__1  great for moving to different areas\n",
      "__label__1  Wish it had some sort of non skid pad on bottom though\n",
      "__label__1 Both sides are magnetic\n",
      "__label__1  A real plus when you're entertaining more than one child\n",
      "__label__1  The four-year old can find the letters for the words, while the two-year old can find the pictures the words spell\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# Function below unzips the archive to the local directory.\n",
    "\n",
    "\n",
    "def unzip_data(input_file_path: str, output_file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Unzip the data file\n",
    "    \"\"\"\n",
    "    # extracl all files to the output_file_path, that already includes the file name\n",
    "    with zipfile.ZipFile(input_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_file_path)\n",
    "\n",
    "\n",
    "# Input data is a file with a single JSON object per line with the following format:\n",
    "# {\n",
    "#  \"reviewerID\": <string>,\n",
    "#  \"asin\": <string>,\n",
    "#  \"reviewerName\" <string>,\n",
    "#  \"helpful\": [\n",
    "#    <int>, (indicating number of \"helpful votes\")\n",
    "#    <int>  (indicating total number of votes)\n",
    "#  ],\n",
    "#  \"reviewText\": \"<string>\",\n",
    "#  \"overall\": <int>,\n",
    "#  \"summary\": \"<string>\",\n",
    "#  \"unixReviewTime\": <int>,\n",
    "#  \"reviewTime\": \"<string>\"\n",
    "# }\n",
    "#\n",
    "# We are specifically interested in the fields \"helpful\" and \"reviewText\"\n",
    "#\n",
    "\n",
    "\n",
    "def label_data(input_data_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Label the data based on the review score\n",
    "    \"\"\"\n",
    "    labeled_data = []\n",
    "    HELPFUL_LABEL = \"__label__1\"\n",
    "    UNHELPFUL_LABEL = \"__label__2\"\n",
    "\n",
    "    for line in open(input_data_path, \"r\"):\n",
    "        l_object = json.loads(line)\n",
    "        helpful_votes = l_object[\"helpful\"][0]\n",
    "        total_votes = l_object[\"helpful\"][1]\n",
    "        review_text = l_object[\"reviewText\"]\n",
    "\n",
    "        if total_votes != 0:\n",
    "            if helpful_votes / total_votes >= 0.5:\n",
    "                labeled_data.append(f\"{HELPFUL_LABEL} {review_text}\\n\")\n",
    "            elif helpful_votes / total_votes < 0.5:\n",
    "                labeled_data.append(f\"{UNHELPFUL_LABEL} {review_text}\\n\")\n",
    "\n",
    "    return labeled_data\n",
    "\n",
    "\n",
    "# Labeled data is a list of sentences, starting with the label defined in label_data.\n",
    "\n",
    "\n",
    "def split_sentences(labeled_data):\n",
    "    split_sentences = []\n",
    "    for d in labeled_data:\n",
    "        label = d.split()[0]\n",
    "        sentences = \" \".join(d.split()[1:]).split(\n",
    "            \".\"\n",
    "        )  # Initially split to separate label, then separate sentences\n",
    "        for s in sentences:\n",
    "            if s:  # Make sure sentences isn't empty. Common w/ \"...\"\n",
    "                split_sentences.append(\" \".join([label, s]))\n",
    "    return split_sentences\n",
    "\n",
    "\n",
    "input_data = unzip_data(DATA_FILE_PATH, TRANSFORMED_DATA_DIR)\n",
    "labeled_data = label_data(OUTPUT_FILE_PATH)\n",
    "split_sentence_data = split_sentences(labeled_data)\n",
    "\n",
    "# print 6 first sentences, one per line\n",
    "print(\"\\n\".join(split_sentence_data[:6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478252a7",
   "metadata": {},
   "source": [
    "## Exercise: Upload Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0724810e",
   "metadata": {},
   "source": [
    "Your first responsibility is to separate that `split_sentence_data` into a `training_file` and a `validation_file`. Have the training file make up 90% of the data, and have the validation file make up 10% of the data. Careful that the data doesn't overlap! (This will result in overfitting, which might result in nice validation metrics, but crummy generalization.)\n",
    "\n",
    "Using the methodology of your choice, upload these files to S3. (In practice, it's important to know how to do this through the console, programatically, and through the CLI. If you're feeling frisky, try all 3!) If you're doing this programatically, the Boto3 documentation would be a good reference. https://boto3.amazonaws.com/v1/documentation/api/latest/index.html\n",
    "\n",
    "The BUCKET will be the name of the bucket you wish to upload it to. The s3_prefix will be the name of the desired 'file-path' that you upload your file to within the bucket. For example, if you wanted to upload a file to:\n",
    "\n",
    "\"s3://example-bucket/1/2/3/example.txt\n",
    "\n",
    "The \"BUCKET\" will be 'example-bucket', and the s3_prefix would be '1/2/3'\n",
    "\n",
    "The code below shows you how to upload it programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6820e8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file written!\n",
      "Validation file written!\n",
      "Train file uploaded!\n",
      "Validation file uploaded!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Note: This section implies that the bucket below has already been made and that you have access\n",
    "# to that bucket. You would need to change the bucket below to a bucket that you have write\n",
    "# premissions to. This will take time depending on your internet connection, the training file is ~ 40 mb\n",
    "\n",
    "BUCKET = \"sigmoidal-bucket\"\n",
    "s3_prefix = \"1\"\n",
    "\n",
    "\n",
    "def cycle_data(fp, data):\n",
    "    for d in data:\n",
    "        fp.write(d + \"\\n\")\n",
    "\n",
    "\n",
    "def write_trainfile(split_sentence_data):\n",
    "    train_path = \"hello_blaze_train\"\n",
    "    with open(train_path, \"w\") as f:\n",
    "        cycle_data(f, split_sentence_data)\n",
    "    return train_path\n",
    "\n",
    "\n",
    "def write_validationfile(split_sentence_data):\n",
    "    validation_path = \"hello_blaze_validation\"\n",
    "    with open(validation_path, \"w\") as f:\n",
    "        cycle_data(f, split_sentence_data)\n",
    "    return validation_path\n",
    "\n",
    "\n",
    "def upload_file_to_s3(file_name, s3_prefix):\n",
    "    object_name = os.path.join(s3_prefix, file_name)\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, BUCKET, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "\n",
    "\n",
    "# Split the data\n",
    "split_data_trainlen = int(len(split_sentence_data) * 0.9)\n",
    "split_data_validationlen = int(len(split_sentence_data) * 0.1)\n",
    "\n",
    "# Todo: write the training file\n",
    "train_path = write_trainfile(split_sentence_data[:split_data_trainlen])\n",
    "print(\"Training file written!\")\n",
    "\n",
    "# Todo: write the validation file\n",
    "validation_path = write_validationfile(split_sentence_data[split_data_trainlen:])\n",
    "print(\"Validation file written!\")\n",
    "\n",
    "upload_file_to_s3(train_path, s3_prefix)\n",
    "print(\"Train file uploaded!\")\n",
    "upload_file_to_s3(validation_path, s3_prefix)\n",
    "print(\"Validation file uploaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f103ba51",
   "metadata": {},
   "source": [
    "## Exercise: Train SageMaker Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd8b876",
   "metadata": {},
   "source": [
    "Believe it or not, you're already almost done! Part of the appeal of SageMaker is that AWS has already done the heavy implementation lifting for you. Launch a \"BlazingText\" training job from the SageMaker console. You can do so by searching \"SageMaker\", and navigating to Training Jobs on the left hand side. After selecting \"Create Training Job\", perform the following steps:\n",
    "* Select \"BlazingText\" from the algorithms available. \n",
    "* Specify the \"file\" input mode of training. \n",
    "* Under \"resource configuration\", select the \"ml.m5.large\" instance type. Specify 5 additional GBs of memory. \n",
    "* Set a stopping condition for 15 minutes. \n",
    "* Under hyperparameters, set \"mode\" to \"supervised\"\n",
    "* Under input_data configuration, input the S3 path to your training and validation datasets under the \"train\" and \"validation\" channels. You will need to create a channel named \"validation\".  \n",
    "* Specify an output path in the same bucket that you uploaded training and validation data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import logging\n",
    "\n",
    "# load BUCKET and ROLE from .env file\n",
    "bucket = os.getenv(\"BUCKET\")\n",
    "role = os.getenv(\"ROLE\")\n",
    "\n",
    "S3_LOCATION = f\"s3://{bucket}/1\"\n",
    "\n",
    "# By default, The SageMaker SDK logs events related to the default\n",
    "# configuration using the INFO level. To prevent these from spoiling\n",
    "# the output of this notebook cells, we can change the logging\n",
    "# level to ERROR instead.\n",
    "logging.getLogger(\"sagemaker.config\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "pipeline_session = PipelineSession(default_bucket=bucket)\n",
    "\n",
    "config = {\n",
    "    \"session\": pipeline_session,\n",
    "    \"instance_type\": \"ml.m5.xlarge\",\n",
    "    \"framework_version\": \"2.11\",\n",
    "    \"py_version\": \"py39\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "iam_client = boto3.client(\"iam\")\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "WARNING:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: blazingtext-2023-12-13-13-50-26-062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-13 13:50:27 Starting - Starting the training job...\n",
      "2023-12-13 13:50:41 Starting - Preparing the instances for training......\n",
      "2023-12-13 13:51:48 Downloading - Downloading input data...\n",
      "2023-12-13 13:52:29 Training - Training image download completed. Training in progress..Arguments: train\n",
      "/opt/amazon/lib/python3.8/site-packages/blazingtext/train_methods.py:176: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if p.poll() is not 0:\n",
      "/opt/amazon/lib/python3.8/site-packages/blazingtext/train_methods.py:253: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if p.poll() is not 0:\n",
      "/opt/amazon/lib/python3.8/site-packages/blazingtext/train_methods.py:326: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if p.poll() is not 0:\n",
      "[12/13/2023 13:52:36 WARNING 140499039393600] Loggers have already been setup.\n",
      "[12/13/2023 13:52:36 WARNING 140499039393600] Loggers have already been setup.\n",
      "/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "[12/13/2023 13:52:36 INFO 140499039393600] nvidia-smi took: 0.025156259536743164 secs to identify 0 gpus\n",
      "[12/13/2023 13:52:36 INFO 140499039393600] Running single machine CPU BlazingText training using supervised mode.\n",
      "Number of CPU sockets found in instance is  1\n",
      "[12/13/2023 13:52:36 INFO 140499039393600] Processing /opt/ml/input/data/train/hello_blaze_train . File size: 44.97289848327637 MB\n",
      "[12/13/2023 13:52:36 INFO 140499039393600] Processing /opt/ml/input/data/validation/hello_blaze_validation . File size: 5.058439254760742 MB\n",
      "Read 8M words\n",
      "Number of words:  35179\n",
      "Loading validation data from /opt/ml/input/data/validation/hello_blaze_validation\n",
      "Loaded validation data.\n",
      "##### Alpha: 0.0485  Progress: 2.98%  Million Words/sec: 6.25 #####\n",
      "##### Alpha: 0.0456  Progress: 8.71%  Million Words/sec: 6.87 #####\n",
      "-------------- End of epoch: 1\n",
      "##### Alpha: 0.0427  Progress: 14.50%  Million Words/sec: 7.05 #####\n",
      "##### Alpha: 0.0399  Progress: 20.25%  Million Words/sec: 7.11 #####\n",
      "-------------- End of epoch: 2\n",
      "##### Alpha: 0.0370  Progress: 26.05%  Million Words/sec: 7.16 #####\n",
      "-------------- End of epoch: 3\n",
      "##### Alpha: 0.0341  Progress: 31.75%  Million Words/sec: 7.17 #####\n",
      "##### Alpha: 0.0312  Progress: 37.54%  Million Words/sec: 7.19 #####\n",
      "-------------- End of epoch: 4\n",
      "##### Alpha: 0.0284  Progress: 43.25%  Million Words/sec: 7.20 #####\n",
      "##### Alpha: 0.0255  Progress: 48.98%  Million Words/sec: 7.20 #####\n",
      "-------------- End of epoch: 5\n",
      "Using 4 threads for prediction!\n",
      "Validation accuracy: 0.825823\n",
      "Validation accuracy improved! Storing best weights...\n",
      "##### Alpha: 0.0228  Progress: 54.36%  Million Words/sec: 6.52 #####\n",
      "##### Alpha: 0.0199  Progress: 60.13%  Million Words/sec: 6.59 #####\n",
      "-------------- End of epoch: 6\n",
      "Using 4 threads for prediction!\n",
      "Validation accuracy: 0.796018\n",
      "Validation accuracy has not improved for last 1 epochs.\n",
      "##### Alpha: 0.0171  Progress: 65.79%  Million Words/sec: 6.52 #####\n",
      "-------------- End of epoch: 7\n",
      "Using 4 threads for prediction!\n",
      "Validation accuracy: 0.807025\n",
      "Validation accuracy has not improved for last 2 epochs.\n",
      "##### Alpha: 0.0145  Progress: 70.98%  Million Words/sec: 6.49 #####\n",
      "##### Alpha: 0.0118  Progress: 76.49%  Million Words/sec: 6.52 #####\n",
      "-------------- End of epoch: 8\n",
      "Using 4 threads for prediction!\n",
      "Validation accuracy: 0.792726\n",
      "Validation accuracy has not improved for last 3 epochs.\n",
      "##### Alpha: 0.0089  Progress: 82.24%  Million Words/sec: 6.48 #####\n",
      "##### Alpha: 0.0060  Progress: 88.01%  Million Words/sec: 6.53 #####\n",
      "-------------- End of epoch: 9\n",
      "Using 4 threads for prediction!\n",
      "Validation accuracy: 0.810815\n",
      "Validation accuracy has not improved for last 4 epochs.\n",
      "Reached patience. Terminating training.\n",
      "Best epoch: 5\n",
      "Best validation accuracy: 0.825823\n",
      "##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 7.15 #####\n",
      "Training finished.\n",
      "Average throughput in Million words/sec: 7.15\n",
      "Total training time in seconds: 12.40\n",
      "#train_accuracy: 0.8717\n",
      "Number of train examples: 470152\n",
      "#validation_accuracy: 0.8258\n",
      "Number of validation examples: 52240\n",
      "\n",
      "2023-12-13 13:53:04 Uploading - Uploading generated training model\n",
      "2023-12-13 13:53:30 Completed - Training job completed\n",
      "Training seconds: 102\n",
      "Billable seconds: 102\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "\n",
    "# Specify the S3 paths for training and validation data\n",
    "train_path = f\"{S3_LOCATION}/hello_blaze_train\"\n",
    "validation_path = f\"{S3_LOCATION}/hello_blaze_validation\"\n",
    "\n",
    "# Get the BlazingText Docker image\n",
    "container = get_image_uri(region, \"blazingtext\", repo_version=\"latest\")\n",
    "\n",
    "\n",
    "# Create a BlazingText Estimator\n",
    "blazingtext = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=f\"{S3_LOCATION}/model_artifacts\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "# Set Hyperparameters (adjust these as needed)\n",
    "blazingtext.set_hyperparameters(\n",
    "    mode=\"supervised\",\n",
    "    epochs=10,\n",
    "    vector_dim=100,\n",
    "    early_stopping=True,\n",
    "    patience=4,\n",
    "    min_epochs=5,\n",
    "    learning_rate=0.05,\n",
    ")\n",
    "\n",
    "# Set the input data channels\n",
    "train_data = TrainingInput(train_path, content_type=\"text/plain\")\n",
    "validation_data = TrainingInput(validation_path, content_type=\"text/plain\")\n",
    "data_channels = {\"train\": train_data, \"validation\": validation_data}\n",
    "\n",
    "# Fit the Estimator\n",
    "blazingtext.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44a48c1d",
   "metadata": {},
   "source": [
    "If successful, you should see a training job launch in the UI. Go grab a coffee, this will take a little bit of time. If there was a failure, you should see it there. Googling the error should direct "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2604071",
   "metadata": {},
   "source": [
    "## Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429bb72f",
   "metadata": {},
   "source": [
    "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering  \n",
    "R. He, J. McAuley  \n",
    "WWW, 2016\n",
    "\n",
    "\n",
    "Image-based recommendations on styles and substitutes  \n",
    "J. McAuley, C. Targett, J. Shi, A. van den Hengel  \n",
    "SIGIR, 2015\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
